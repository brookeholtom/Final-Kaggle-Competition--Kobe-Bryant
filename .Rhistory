folds <- vfold_cv(augustine, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low, -Occupancy, -Average)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
RMSE = sqrt(nanmean((OccAvg_predicted-OccAvg).^2))
1 - mse / Var(OccAvg)
mse(predictions_tree_augustine)
performance_mse(predictions_tree_augustine)
summary(final_wf_tree)
final_wf_tree
performance::performance_mse(predictions_tree_augustine)
library(performance)
performance::mse(predictions_tree_augustine)
performance::mse(final_wf_tree)
CV_results_tree_augustine
bestTune_augustine
predictions_tree_augustine
(predictions_tree_augustine - augustine_tree)
predictions_tree_augustine
augustine_tree
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)^2/75
sum(predictions_tree_augustine - augustine_tree$OccAvg)
sum(predictions_tree_augustine - augustine_tree$OccAvg)^2
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)/75
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
LR_R
RSqaured_Augustine
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
RSquared_Augustine = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSquared_Augustine
knitr::opts_chunk$set(echo = TRUE)
###Loading Packages###
library(tidyverse)
library(corrplot)
library(bestglm)
library(car)
library(vroom)
library(tidymodels)
view(augustine)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
view(augustine)
mutate(3mAvg = (DaysBooked3Months/92)*Average %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
augustine <- augustine %>%
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
augustine <- augustine %>%
###EDA###
augustine <- augustine %>%
mutate(BookedAvg = (DaysBookedinNext30/30)*Average) %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
select(-KidFriendly, -ClimateControl, -FastWifi, -KitchenEssentials, -'W&D')
ggplot(data = augustine) +
mutate(ThreeAvg = (DaysBooked3Months/92)*Average %>%
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
#Imputation
train_missing <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/trainWithMissingValues.csv")
train <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/train.csv")
test <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/test.csv")
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune_nb <- CV_results_nb %>%
select_best("accuracy")
final_wf_nb <-
nb_wf %>%
finalize_workflow(bestTune_nb) %>%
fit(data=train)
predictions_nb <- final_wf_nb %>%
predict(test, type = "class")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
vroom_write(x= predictions_nb, file="predictions_nb_2.csv", delim=",")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune_nb <- CV_results_nb %>%
select_best("accuracy")
final_wf_nb <-
nb_wf %>%
finalize_workflow(bestTune_nb) %>%
fit(data=train)
predictions_nb <- final_wf_nb %>%
predict(test, type = "class")
predictions_nb
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
vroom_write(x= predictions_nb, file="predictions_nb_2.csv", delim=",")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
#update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
library(patchwork)
library(timetk)
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
library(forecast)
library(modeltime)
train <- vroom("data.csv") %>%
na.omit(shot_made_flag)
setwd("C:/Users/brook/Downloads/STAT348/KobeBryant")
train <- vroom("data.csv") %>%
na.omit(shot_made_flag)
test <- vroom("data.csv") %>%
filter(is.na(shot_made_flag))
#Random Forest
my_recipe <- recipe(shot_made_flag ~., data=train) %>%
step_date(game_date, features="doy") %>%
step_date(game_date, features="dow") %>%
step_mutate_at(all_nominal_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(shot_made_flag)) %>%
step_naomit()
prep <- prep(my_recipe)
#Random Forest
my_recipe <- recipe(shot_made_flag ~., data=train) %>%
step_date(game_date, features="doy") %>%
step_date(game_date, features="dow") %>%
step_mutate_at(all_nominal_predictors(), fn = factor) %>%
#step_lencode_mixed(all_nominal_predictors(), outcome=vars(shot_made_flag)) %>%
step_naomit()
prep <- prep(my_recipe)
bake(prep, new_data = train)
my_mod_forest <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>%
set_engine("ranger") %>%
set_mode("regression")
## Create a workflow with model & recipe
workflow_forest <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod_forest)
## Set up grid of tuning values
tuning_grid <- grid_regular(mtry(range=c(1, ncol(train) - 1)),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_forest <- workflow_forest %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(mae))
## Find best tuning parameters
bestTune <- CV_results_forest %>%
select_best("mae")
collect_metrics(CV_results_forest)
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data=train)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data=train)
workflow_forest
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data=train)
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data=train)
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data = train)
final_wf_forest <-
workflow_forest %>%
finalize_workflow(train) %>%
fit(data = train)
final_wf_forest <- workflow_forest %>%
finalize_workflow(train) %>%
fit(data = train)
final_wf_forest <- workflow_forest %>%
finalize_workflow(train) %>%
fit(data = train)
rlang::last_trace()
final_wf_forest <- workflow_forest %>%
finalize_workflow(bestTune) %>%
fit(data = train)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(test)
predictions_forest <- final_wf_forest %>%
predict(test)
predictions_forest <- final_wf_forest %>%
na.omit(action_type) %>%
predict(test)
predictions_forest <- final_wf_forest %>%
predict(test)
predictions_forest <- predictions_forest %>%
predict(test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob") %>%
bind_cols(., test) %>%
select(shot_id, .pred_1) %>%
rename(shot_made_flag = .pred_1)
predictions_forest <- final_wf_forest %>%
predict(test, type = "class") %>%
bind_cols(., test) %>%
select(shot_id, .pred_1) %>%
rename(shot_made_flag = .pred_1)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob") %>%
bind_cols(., test) %>%
select(shot_id, .pred_1) %>%
rename(shot_made_flag = .pred_1)
rlang::last_trace()
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob") %>%
fit(data = train)
final_wf_forest <- workflow_forest %>%
finalize_workflow(bestTune) %>%
fit(data = train)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(new_data = test, type = "prob")
view(test)
rlang::last_trace()
bake(prep, new_data=test)
bake(prep, new_data=test)
predictions_forest <- final_wf_forest %>%
predict(new_data = test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(new_data = test, type = "class")
predictions_forest <- final_wf_forest %>%
predict(new_data = test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
predictions_forest <- final_wf_forest %>%
predict(test)
predictions_forest <- final_wf_forest %>%
predict(test)
train <- vroom("data.csv") %>%
na.omit(shot_made_flag) %>%
select(-action_type)
test <- vroom("data.csv") %>%
filter(is.na(shot_made_flag))
test <- vroom("data.csv") %>%
filter(is.na(shot_made_flag)) %>%
select(-action_type)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
test <- vroom("data.csv") %>%
filter(is.na(shot_made_flag))
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
view(train)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
final_wf_forest <- workflow_forest %>%
finalize_workflow(bestTune) %>%
fit(data = train)
final_wf_forest <- workflow_forest %>%
finalize_workflow(bestTune) %>%
fit(data = train)
test <- vroom("data.csv") %>%
filter(is.na(shot_made_flag))
train <- vroom("data.csv") %>%
na.omit(shot_made_flag)
prep <- prep(my_recipe)
bake(prep, new_data = train)
bake(prep, new_data=test)
final_wf_forest <- workflow_forest %>%
finalize_workflow(bestTune) %>%
fit(data = train)
predictions_forest <- final_wf_forest %>%
predict(test, type = "prob")
rlang::last_trace()
predictions_forest <- final_wf_forest %>%
predict(test)
train <- vroom("data.csv") %>%
setwd("C:/Users/brook/Downloads/STAT348/KobeBryant")
train <- vroom("data.csv") %>%
library(patchwork)
library(timetk)
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
library(forecast)
library(modeltime)
train <- vroom("data.csv") %>%
library(patchwork)
library(timetk)
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
library(forecast)
library(modeltime)
train <- vroom("data.csv") %>%
train <- vroom("data.csv") %>%
select((-c('team_id', 'team_name', 'shot_zone_range', 'lon', 'lat',
'game_id', 'game_date','shot_zone_area',
